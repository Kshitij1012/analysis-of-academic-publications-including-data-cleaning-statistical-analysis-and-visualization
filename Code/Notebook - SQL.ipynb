{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc0f6ff-e5ef-4f5a-83a3-c07ac3621301",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Large_file_path = \"dbfs:/FileStore/tables/large.json.gz\"\n",
    "\n",
    "Journal_file_path = \"dbfs:/FileStore/tables/journal_information.csv\"\n",
    "\n",
    "journal_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(Journal_file_path)\n",
    "large_df = spark.read.format(\"json\").load(Large_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ed0593-6367-476c-a166-9b22e5ed44e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Registering DataFrame as temporary SQL tables\n",
    "large_table =large_df.createOrReplaceTempView(\"large_table\")\n",
    "journal_table= journal_df.createOrReplaceTempView(\"journal_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41991f4b-eda9-4c1c-bcc8-e9a17ab322ed",
     "showTitle": true,
     "title": "1. Programmatically confirm that all papers have unique IDs and output the number of papers in the file."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 150000\nNumber of unique IDs: 150000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1. Programmatically confirm that all papers have unique IDs and output the number of papers in the file.\"\"\"\n",
    "def check_unique_ids_sql(large_table):\n",
    "\n",
    "    # SQL query to count distinct CorpusId and total papers\n",
    "    Distinct_corpusID_papers = spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT CorpusId) AS unique_ids_count, COUNT(*) AS total_papers_count\n",
    "    FROM large_table\n",
    "    \"\"\")\n",
    "    # Executing SQL query and collect the result\n",
    "    result = Distinct_corpusID_papers.collect()[0]\n",
    "\n",
    "    # Extracting unique IDs count and total papers count\n",
    "    unique_ids_count = result['unique_ids_count']\n",
    "    total_papers_count = result['total_papers_count']\n",
    "\n",
    "    print(\"Total number of papers:\", total_papers_count)\n",
    "    print(\"Number of unique IDs:\", unique_ids_count)\n",
    "\n",
    "check_unique_ids_sql(large_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772a211a-2570-4cec-9aa8-ccd45e868443",
     "showTitle": true,
     "title": "2. What is the average number of authors per paper?"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of authors per paper: 2.81628\n"
     ]
    }
   ],
   "source": [
    "\"\"\"2. What is the average number of authors per paper?\"\"\"\n",
    "def calculate_avg_authors_per_paper_sql(large_table):\n",
    "\n",
    "    # SQL query to calculate the average number of authors per paper\n",
    "    Avg_authors = spark.sql(\"\"\"\n",
    "    SELECT AVG(size(authors)) AS avg_authors_per_paper\n",
    "    FROM large_table\n",
    "    \"\"\")\n",
    "\n",
    "    # to collect the result\n",
    "    avg_authors_per_paper = Avg_authors.collect()[0]['avg_authors_per_paper']\n",
    "\n",
    "    print(\"Average number of authors per paper:\", avg_authors_per_paper)\n",
    "\n",
    "calculate_avg_authors_per_paper_sql(large_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17160594-9d40-42b2-a13b-759d4d1db0af",
     "showTitle": true,
     "title": " 3. How many different journals were the papers published in?"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different journals: 33916\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 3. How many different journals were the papers published in?\"\"\"\n",
    "def count_distinct_journals_sql(large_table):\n",
    "\n",
    "    # Write SQL query to filter out null values and empty strings in the \"journal\" column\n",
    "    journal_filter = spark.sql(\"\"\"\n",
    "        SELECT COUNT(DISTINCT journal.name) AS distinct_journals_count\n",
    "        FROM large_table\n",
    "        WHERE journal.name IS NOT NULL AND journal.name != ''\n",
    "    \"\"\")\n",
    "\n",
    "    # to collect the result\n",
    "    distinct_journals_count = journal_filter.collect()[0][0]\n",
    "    print(\"Number of different journals:\", distinct_journals_count)\n",
    "\n",
    "count_distinct_journals_sql(large_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ec0dd4-b2cb-437c-af8d-1c719c5f60fa",
     "showTitle": true,
     "title": "4. Find the 5 authors with the highest number of publications. Give their names along with the number of publications they contributed to."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------------+\n|authorId  |name           |publication_count|\n+----------+---------------+-----------------+\n|2149377746|B. Noble       |23               |\n|90537224  |S. Sukhoruchkin|16               |\n|88842366  |Z. Soroko      |16               |\n|49898687  |M. Kumar       |15               |\n|49611617  |M. Jain        |10               |\n+----------+---------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"4. Find the 5 authors with the highest number of publications. \n",
    "    Give their names along with the number of publications they contributed to.\"\"\"\n",
    "def top_authors_by_publication_count_sql(large_table):\n",
    "\n",
    "    # SQL query to explode the authors array, group by author, and count the number of publications per author\n",
    "    count_publication = spark.sql(\"\"\"\n",
    "    SELECT author.authorId, author.name, COUNT(*) AS publication_count\n",
    "    FROM large_table\n",
    "    LATERAL VIEW explode(authors) AS author\n",
    "    GROUP BY author.authorId, author.name\n",
    "    \"\"\")\n",
    "\n",
    "    # selecting the top 5 authors\n",
    "    top_10_authors = count_publication.orderBy(count_publication[\"publication_count\"].desc()).limit(5)\n",
    "\n",
    "    # Display the top 5 authors with the highest number of publications\n",
    "    top_10_authors.show(truncate=False)\n",
    "\n",
    "\n",
    "top_authors_by_publication_count_sql(large_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106cac65-4047-43f3-884b-62c9ee6e924d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Created a function for joining of table for simplicity of further analysis \"\"\"\n",
    "def joined_tables (large_table, journal_table):\n",
    "# Filtering out the header row in journal_table\n",
    "    journal_table1 = spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT *, row_number() OVER (ORDER BY NULL) as row_num FROM journal_table\n",
    "        )\n",
    "        WHERE row_num > 1\n",
    "    \"\"\")\n",
    "\n",
    "    # Filtering out null values in journal column and flatten large_table\n",
    "    flattened_table = spark.sql(\"\"\"\n",
    "        SELECT *, journal.name AS journal_name FROM large_table\n",
    "        WHERE journal IS NOT NULL AND journal.name IS NOT NULL\n",
    "    \"\"\")\n",
    "\n",
    "    flattened_table.createOrReplaceTempView(\"flattened_table\")\n",
    "    journal_table1.createOrReplaceTempView(\"journal_table1\")\n",
    "\n",
    "    # Joining flattened_table with journal_table1 based on 'Journal Name'\n",
    "    joined_df = spark.sql(\"\"\"\n",
    "        SELECT s.*, j.* FROM flattened_table s JOIN journal_table1 j\n",
    "        ON s.journal_name = j.`Journal Name`\n",
    "    \"\"\")\n",
    "\n",
    "    # Registering DataFrame as temporary SQL table\n",
    "    joined_df.createOrReplaceTempView(\"joined_table\")\n",
    "\n",
    "joined_tables (large_table, journal_table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb4e527-4990-40e1-b0e3-47babf15288e",
     "showTitle": true,
     "title": "5. To gain some additional information about publication quality, youâ€™d like to join the paper information with the journal information you have. Following this, find the top 5 authors with the highest cummulative impact factor (notice that journals have different impact factors listed in the journal file in the IF column). Output both the author information and the cummulative impact factor."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+\n|  authorId| author_name|cumulative_IF|\n+----------+------------+-------------+\n|2155504929|     Ying Li|       93.832|\n| 144797099|    M. Viana|       92.238|\n|   5152451|  L. Andrade|       92.238|\n|  49900836|     H. Wood|       90.422|\n|   7695437|A. M. Ruscio|       87.899|\n+----------+------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"5. Find the top 5 authors with the highest cummulative impact factor.\n",
    "       Output both the author information and the cummulative impact factor.\"\"\"\n",
    "\n",
    "# Calculating cumulative impact factor for each author\n",
    "author_impact_factor = spark.sql(\"\"\"\n",
    "    SELECT exploded_author.authorId AS authorId_exploded, \n",
    "            exploded_author.name AS author_name_exploded, \n",
    "            COALESCE(SUM(joined_table.IF), 0) AS cumulative_impact_factor\n",
    "    FROM joined_table\n",
    "    LATERAL VIEW explode(authors) exploded_table AS exploded_author\n",
    "    WHERE joined_table.journal_name IS NOT NULL\n",
    "    GROUP BY exploded_author.authorId, exploded_author.name\n",
    "\"\"\")\n",
    "\n",
    "author_impact_factor.createOrReplaceTempView(\"author_impact_factor\")\n",
    "\n",
    "# top 5 authors with the highest cumulative impact factor\n",
    "top_authors_with_IF = spark.sql(\"\"\"\n",
    "    SELECT authorId_exploded AS authorId, \n",
    "            author_name_exploded AS author_name, \n",
    "            SUM(cumulative_impact_factor) AS cumulative_IF\n",
    "    FROM author_impact_factor\n",
    "    GROUP BY authorId_exploded, author_name_exploded\n",
    "    ORDER BY cumulative_IF DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "top_authors_with_IF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7859d81a-efe6-4882-a651-7dd45275ab6e",
     "showTitle": true,
     "title": "6. Youâ€™d like some additional information about publication trends. How many publications with impact factor > 1 were published in each of the years between 2010-2020?"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n|year|count|\n+----+-----+\n|2010|  112|\n|2011|  139|\n|2012|  165|\n|2013|  178|\n|2014|  241|\n|2015|  243|\n|2016|  283|\n|2017|  329|\n|2018|  365|\n|2019|  396|\n|2020|  444|\n+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"6. Youâ€™d like some additional information about publication trends. \n",
    "        How many publications with impact factor > 1 were published in each of the years between 2010-2020?\"\"\"\n",
    "\n",
    "# Filter publications with impact factor > 1 and publication year between 2010 and 2020\n",
    "filtered_df = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM joined_table\n",
    "    WHERE CAST(IF AS FLOAT) > 1 AND year BETWEEN 2010 AND 2020\n",
    "\"\"\")\n",
    "# Register filtered_df as temporary SQL table\n",
    "filtered_df.createOrReplaceTempView(\"filtered_table\")\n",
    "\n",
    "# Group by publication year and count the number of publications\n",
    "publications_per_year = spark.sql(\"\"\"\n",
    "    SELECT year, COUNT(*) AS count\n",
    "    FROM filtered_table\n",
    "    GROUP BY year\n",
    "    ORDER BY year\n",
    "\"\"\")\n",
    "\n",
    "publications_per_year.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment - SQL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
