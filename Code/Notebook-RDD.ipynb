{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee8139d-d9d3-436b-a38c-9eef30ce5d28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Large_file_path = \"dbfs:/FileStore/tables/large.json.gz\"\n",
    "\n",
    "Journal_file_path = \"dbfs:/FileStore/tables/journal_information.csv\"\n",
    "\n",
    "journal_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(Journal_file_path)\n",
    "large_df = spark.read.format(\"json\").load(Large_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377f9eef-04a5-4a6b-a319-fe2675e80b76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Convert DataFrame objects (large_df and journal_df) into Resilient Distributed Datasets (RDDs)\n",
    "large_rdd = large_df.rdd\n",
    "journal_rdd = journal_df.rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56abe9c3-dec2-4498-a4f6-1c11a2da1523",
     "showTitle": true,
     "title": "1. Programmatically confirm that all papers have unique IDs and output the number of papers in the file."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 150000\nNumber of unique papers: 150000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1. Programmatically confirm that all papers have unique IDs and output the number of papers in the file.\"\"\"\n",
    "def analyze_rdd_papers(rdd):\n",
    "   \n",
    "    # Extracting the \"corpusid\" column from each row\n",
    "    corpus_ids = rdd.map(lambda row: row[\"corpusid\"])\n",
    "\n",
    "    # Count the total number of papers\n",
    "    total_papers_count = corpus_ids.count()\n",
    "\n",
    "    # Count the number of distinct corpus IDs\n",
    "    unique_papers_count = corpus_ids.distinct().count()\n",
    "\n",
    "    print(\"Total number of papers:\", total_papers_count)\n",
    "    print(\"Number of unique papers:\", unique_papers_count)\n",
    "    \n",
    "analyze_rdd_papers(large_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97efaea-98d4-4acf-843f-72fe9b9e3adf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca0dc83-a05a-4196-a068-ec05846180f1",
     "showTitle": true,
     "title": "2. What is the average number of authors per paper?"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of authors per paper: 2.81628\n"
     ]
    }
   ],
   "source": [
    "\"\"\"2. What is the average number of authors per paper?\"\"\"\n",
    "def calculate_avg_authors_per_paper_rdd(rdd):\n",
    "\n",
    "    # Calculating the number of authors for each paper\n",
    "    authors_per_paper_rdd = rdd.map(lambda row: len(row.authors))\n",
    "\n",
    "    # Calculating the total number of authors and the sum of authors per paper\n",
    "    total_authors_count = authors_per_paper_rdd.count()\n",
    "    total_authors_sum = authors_per_paper_rdd.sum()\n",
    "\n",
    "    # Calculating the average number of authors per paper\n",
    "    avg_authors_per_paper = total_authors_sum / total_authors_count\n",
    "\n",
    "    print(\"Average number of authors per paper:\", avg_authors_per_paper)\n",
    "\n",
    "calculate_avg_authors_per_paper_rdd(large_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9d32081-cf29-4a57-8023-84be522723e2",
     "showTitle": true,
     "title": " 3. How many different journals were the papers published in?"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different journals: 33916\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 3. How many different journals were the papers published in?\"\"\"\n",
    "def count_different_journals(rdd):\n",
    "    \n",
    "    # Filtering out rows with None values in the \"journal\" column\n",
    "    cleaned_rdd = rdd.filter(lambda row: row[\"journal\"] is not None and \n",
    "                             row[\"journal\"][\"name\"] is not None and row[\"journal\"][\"name\"] != \"\")\n",
    "\n",
    "    # Extracting the journal names\n",
    "    journal_names_rdd = cleaned_rdd.map(lambda row: row[\"journal\"][\"name\"])\n",
    "\n",
    "    # Count the number of distinct journal names\n",
    "    distinct_journals_count = journal_names_rdd.distinct().count()\n",
    "\n",
    "    print(\"Number of different journals:\", distinct_journals_count)\n",
    "\n",
    "count_different_journals(large_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff6933b-c0ed-493d-9837-fd1896cb04d2",
     "showTitle": true,
     "title": "4. Find the 5 authors with the highest number of publications. Give their names along with the number of publications they contributed to."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: B. Noble ( 2149377746 ) - Number of Publications: 23\nAuthor: S. Sukhoruchkin ( 90537224 ) - Number of Publications: 16\nAuthor: Z. Soroko ( 88842366 ) - Number of Publications: 16\nAuthor: M. Kumar ( 49898687 ) - Number of Publications: 15\nAuthor: Anonymous ( None ) - Number of Publications: 10\n"
     ]
    }
   ],
   "source": [
    "\"\"\"4. Find the 5 authors with the highest number of publications. \n",
    "    Give their names along with the number of publications they contributed to.\"\"\"\n",
    "def top_authors_by_publications(rdd):\n",
    "    \n",
    "    # Explode the authors array to have one row per author\n",
    "    authors_rdd = rdd.flatMap(lambda row: [(author[\"authorId\"], author[\"name\"]) for author in row[\"authors\"]])\n",
    "\n",
    "    # Mapping each author to a tuple with authorId as key and 1 as value for counting\n",
    "    author_counts_rdd = authors_rdd.map(lambda author: (author, 1))\n",
    "\n",
    "    # Using Reduce by key to count the no of publications/author\n",
    "    author_publication_counts_rdd = author_counts_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "    # Swapping the key-value pairs to sort by publication count\n",
    "    sorted_author_publication_counts_rdd = author_publication_counts_rdd.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "    # Sorting the publication count in descending order\n",
    "    sorted_author_publication_counts_rdd = sorted_author_publication_counts_rdd.sortByKey(ascending=False)\n",
    "\n",
    "    top_5_authors_rdd = sorted_author_publication_counts_rdd.take(5)\n",
    "\n",
    "    for count, author in top_5_authors_rdd:\n",
    "        print(\"Author:\", author[1], \"(\", author[0], \") - Number of Publications:\", count)\n",
    "\n",
    "top_authors_by_publications(large_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e106cdb-4ed2-4f06-804a-48c35a8dba03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Journal Name: string (nullable = true)\n |-- ISSN: string (nullable = true)\n |-- EISSN: string (nullable = true)\n |-- Category & Journal Quartiles: string (nullable = true)\n |-- Citations: string (nullable = true)\n |-- JCI: string (nullable = true)\n |-- percentageOAGold: string (nullable = true)\n |-- IF: string (nullable = true)\n\nroot\n |-- authors: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- authorId: string (nullable = true)\n |    |    |-- name: string (nullable = true)\n |-- citationcount: long (nullable = true)\n |-- corpusid: long (nullable = true)\n |-- externalids: struct (nullable = true)\n |    |-- ACL: string (nullable = true)\n |    |-- ArXiv: string (nullable = true)\n |    |-- CorpusId: string (nullable = true)\n |    |-- DBLP: string (nullable = true)\n |    |-- DOI: string (nullable = true)\n |    |-- MAG: string (nullable = true)\n |    |-- PubMed: string (nullable = true)\n |    |-- PubMedCentral: string (nullable = true)\n |-- influentialcitationcount: long (nullable = true)\n |-- isopenaccess: boolean (nullable = true)\n |-- journal: struct (nullable = true)\n |    |-- name: string (nullable = true)\n |    |-- pages: string (nullable = true)\n |    |-- volume: string (nullable = true)\n |-- publicationdate: string (nullable = true)\n |-- publicationtypes: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- publicationvenueid: string (nullable = true)\n |-- referencecount: long (nullable = true)\n |-- s2fieldsofstudy: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- category: string (nullable = true)\n |    |    |-- source: string (nullable = true)\n |-- title: string (nullable = true)\n |-- url: string (nullable = true)\n |-- venue: string (nullable = true)\n |-- year: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "journal_df1.printSchema()\n",
    "small_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3695b7c2-6019-444d-82b1-5a43fb28da7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Created a function for joining of rdds for simplicity of further analysis\"\"\"\n",
    "def joined_rdd(rdd, journal_rdd):\n",
    "\n",
    "    # Convert the DataFrame to RDD, skipping the header row\n",
    "    journal_rdd = journal_rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "\n",
    "    # Filter out null values from small_rdd\n",
    "    filtered_rdd = rdd.filter(lambda row: row.journal is not None and row.journal.name is not None)\n",
    "\n",
    "    # Flatten filtered_small_rdd to extract journal name\n",
    "    flattened_rdd = filtered_rdd.map(lambda row: (row.journal.name, row))\n",
    "\n",
    "    # Join with journal_rdd using journal name as key\n",
    "    joined = flattened_rdd.join(journal_rdd.map(lambda row: (row['Journal Name'], row)))\n",
    "   \n",
    "    return joined\n",
    "\n",
    "joined = joined_rdd(large_rdd, journal_rdd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fef98d-eee8-4a43-84bf-951a88a6cfee",
     "showTitle": true,
     "title": "5. To gain some additional information about publication quality, youâ€™d like to join the paper information with the journal information you have. Following this, find the top 5 authors with the highest cummulative impact factor (notice that journals have different impact factors listed in the journal file in the IF column). Output both the author information and the cummulative impact factor."
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author ID: 2155504929\nAuthor Name: Ying Li\nCumulative Impact Factor: 93.832\n\nAuthor ID: 5152451\nAuthor Name: L. Andrade\nCumulative Impact Factor: 92.238\n\nAuthor ID: 144797099\nAuthor Name: M. Viana\nCumulative Impact Factor: 92.238\n\nAuthor ID: 49900836\nAuthor Name: H. Wood\nCumulative Impact Factor: 90.422\n\nAuthor ID: 7695437\nAuthor Name: A. M. Ruscio\nCumulative Impact Factor: 87.899\n\n"
     ]
    }
   ],
   "source": [
    "\"\"\"5. Find the top 5 authors with the highest cummulative impact factor.\n",
    "       Output both the author information and the cummulative impact factor.\"\"\"\n",
    "# Grouping and Aggregating\n",
    "author_impact_rdd = joined.flatMap(lambda x: [(author['authorId'], author['name'], \n",
    "float(x[1][1]['IF'])) for author in x[1][0]['authors'] if x[1][1]['IF'] is not None]) \\\n",
    "    .map(lambda x: ((x[0], x[1]), x[2])) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .map(lambda x: (x[0][0], x[0][1], x[1]))\n",
    "\n",
    "#  Sorting and Selecting Top Authors\n",
    "top_5_authors_rdd = author_impact_rdd.takeOrdered(5, key=lambda x: -x[2])\n",
    "\n",
    "#  Displaying Results\n",
    "for author in top_5_authors_rdd:\n",
    "    print(\"Author ID:\", author[0])\n",
    "    print(\"Author Name:\", author[1])\n",
    "    print(\"Cumulative Impact Factor:\", author[2])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdf0e86-89b7-4760-8605-9db0083c6086",
     "showTitle": true,
     "title": "6. Youâ€™d like some additional information about publication trends. How many publications with impact factor > 1 were published in each of the years between 2010-2020?"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2010 | Number of Publications: 112\nYear: 2011 | Number of Publications: 139\nYear: 2012 | Number of Publications: 165\nYear: 2013 | Number of Publications: 178\nYear: 2014 | Number of Publications: 241\nYear: 2015 | Number of Publications: 243\nYear: 2016 | Number of Publications: 283\nYear: 2017 | Number of Publications: 329\nYear: 2018 | Number of Publications: 365\nYear: 2019 | Number of Publications: 396\nYear: 2020 | Number of Publications: 444\n"
     ]
    }
   ],
   "source": [
    "\"\"\"6. Youâ€™d like some additional information about publication trends. \n",
    "        How many publications with impact factor > 1 were published in each of the years between 2010-2020?\"\"\"\n",
    "\n",
    "filtered_rdd = joined.filter(lambda x: x[1][1]['IF'] is not None \n",
    "                                and x[1][0]['year'] is not None \n",
    "                                and float(x[1][1]['IF']) > 1 \n",
    "                                and 2010 <= int(x[1][0]['year']) <= 2020)\n",
    "\n",
    "year_count_rdd = filtered_rdd.map(lambda x: (int(x[1][0]['year']), 1))\n",
    "\n",
    "publications_per_year_rdd = year_count_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "publications_per_year_rdd = publications_per_year_rdd.sortByKey()\n",
    "\n",
    "result = publications_per_year_rdd.collect()\n",
    "\n",
    "for year, count in result:\n",
    "    print(\"Year:\", year, \"| Number of Publications:\", count)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment-RDD",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
